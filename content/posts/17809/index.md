---
author: alper
categories:
  - english
  - reading
  - science
  - software-engineering
date: "2024-12-26T23:31:13+00:00"
guid: https://alper.nl/dingen/?p=17809
parent_post_id: null
post_id: "17809"
title: ""
aliases:
  - /dingen/2024/12/17809/

---
[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)

I thought I'd dive back into history and read the original paper that started it all. It's somewhat technical about encode/decoder layouts and matrix multiplications. None of the components are super exciting for somebody who's been looking at neural networks for the past decade.

What's exciting is that such a simplification generates results that are _that_ much better and how they came up with it. Unfortunately, they don't write how they found this out.

The paper itself is a bit too abstract so I'm going to look for some of those YouTube videos that explain what is actually going on here and why it's such a big deal. I'll update this later.
